# 概念
##大数据三种主流架构
- Lambda Batch Layer处理离线数据成一个view、Speed Layer处理成一个Realtime View，Serving layer合并两者的结果给用户提供数据
- Kappa 流批一体的处理
- IOTA 设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种Ad-hoc Query(即席查询)来查询底层数据：
- ![image](https://user-images.githubusercontent.com/3971073/185079199-2744c05c-4416-4b19-b2a6-1eabc424c35a.png)


## 什么是数仓
- 面向主题的、集成的、相对稳定，反映历史变化的一个数据集合
- 主要是为企业决策，是查询、分析和决策的基础
## 数据域
它指的是特定行或字段可以包含的可接受值的范围， 例如在职状态，只有在职和离职等这样的信息。
## 事实表类型
- 事务
- 周期快照
- 拉链表， 节省空间并且可以历史回溯，主要是对数据变化有一个截止和开始日期的保存
- 累积快照表
## 数据治理
- 梳理业务流程，规划数据资产 例如电商业务下的订单域、会员域的划分等
- 采集数据、处理数据  例如a+、ut等埋点
- 数据存储 安全 、质量 例如实时+离线+meta管理+数据测试等
- 数据使用olap的 安全和权限隔离性 ：数据相关使用，olap 、olap 等方向

## 数据分层
- ods层 原始数据层
- dwd ods层上主题的简单汇总表
- dws 数据服务层，各个主题的宽表
- ads  数据应用层
- dim层 维度表 维度和事实表不是直接关联，而是级联的称为雪花型
## 星型和雪花型
- 就是描述事实表和维度表的关系的模型
* 星型就是将各种维度数据冗余，制成大宽表， 方便快速聚合操作，但是数据冗余
* 雪花型在基础上对维度表规划化的层级划分，缺点是需要自己关联维度表
## 建模方式
- 范式建模（Inmon）从下往上，主要关注于数据的etl以及数据和实体的关系上，表没有冗余，数据分析需要关联
- 维度建模（kimball）从上往下，主要关注于数据处理成事实表和维度表，面向分析，数据冗余，快速分析
## 常见问题解决方案
### 数据维度缓慢变化
- 直接使用新的维度值，这样会丢失之前的数据
- 添加维度列，这样可以区分变化前后的维度，方便统计分组
- 极限拉链存储 只保留变动的数据的开始时间结束时间，主要为了查询历史和当前，例如用户的地址信息变更，a-北京-2010-2013，a-南京-2013-2019等这样的，方便查看历史
# 实时流-FLINK
- 流批一体的流数据处理引擎
## 组成
###  主要组成
- jm  job管理
- tm 任务管理
- slot 资源复用
### 常用api
- dataset api 
- table api 
- datasteamingapi 
### 特点
 - 积极的内存管理
   * network buffer 网络内存，netty使用，堆外内存
   * Remaining (Free) Heap 给用户和tm数据结构的内存
   * Memory Manager pool 算子和中间结果的存储都是通过这个来进行管理
- 显示的管理内存
   * 批量的申请和使用内存，减少申请内存的消耗
- 定制的序列化
  * 提高传输效率

### 其他知识点
- checkpoint机制
   * checkpoint 是有am触发的在数据流中插入barrier，在各个operator之间记录流转，接受就存储snapshot到backend，目前是的state是rocksdbleveldb 底层是lsm tree日志存储
-  背压机制
    * 之前是通过tm之前的socket进行控制，但是会导致socket的网络堵塞，所以目前改成tm之间的缓存区
- 广播机制
   * 就是在较少的数据共享在tm内存中，减少资源浪费
- 处理模型
    * streaming  流
    * tranmation 处理
    * sink 输出
- 数据分区 8个
    * globlepartition 下游算子的第一个实例
    * rebalancepartition 均衡分区数据
- barrier 对齐
    * 双流join，不等待其他的流来就直接往下游处理
- 时间机制
   * 事件时间
   * 处理时间
   * 注入时间 
- 精准一次的实现
   * checkpoint+两端提交，一次是checkpoint的实现，另外是sink的提交来保证消息的精准一次消费  
   * sink 两段提交（预提交、提交
### 优化
- topn优化 
  * 增加内存大小，减少对磁盘的访问，提高性能
- windows 对数据流按照时间进行划分，主要有固定窗口，滚动窗口，会话窗口
   * 为了解决乱序和延迟问题，有水位线概念
   * Window Assigner 决定数据应该划分到哪个窗口
   * Evictor  数据过滤
   * trigger 触发窗口的计算和清除
   * 通过watermark来实现窗口函数
- watermark
   * 加上eventime来解决消息乱序问题
- 维度表优化
   * partitionJoin 将相同的 join 条件keyhash到同一个节点，这样提高命中率
   * lru  如果维度较大，就缓存+异步查询
   * all 如果维度表少，就全部加载入缓存
- partitionby要加上日期，否则会出现topN会出现错乱问题
- multi_key_value
- sum等agg优化 
    *  minibatch+ localglobal类似于hadoop的combine和reduce，在上一层阶段做批量的聚合，然后再下游进行合并
- count distinct 使用partialfinal 主要两层groupby的自动实现，提高效率  去重的时候才需要shuffle，所以需要并发度提高，这样才可以增加效率或者是前一层聚合 
   *  approx_count_distinct 大数据近似统计 基于HyperLogLog
- 背压 
  + 出现原因sink端并发度不够，需要进行提高并行度
  + 算子资源问题执行较慢，提高并发和资源 性能问题就是要调整tm的内存和参数
  + 用户代码出现异常，导致无法进行下去
  + 数据倾斜，导致节点处理时间过长
## 性能



# Kafka
## 组成
- topic 消息主题
- producer 生产者
- counsumer 消费者
- consumer group 消费组，一组消费者，如果有变动，会进行rebalance，协调消费者之前的关系
   * 其他的场景有 toipc数量或者是分区数发生变化，都可以引起
    *   join 和sync 操作
 - join操作是加入到队列中，然后coordinator 接受消息，选举一个consumer为leader，然后leader 分配 topic和分区信息，广播到其他的consumer
- broker 负责数据存储和转发
- isr 副本同步队列
- AR是全部队列
- partition 分区
- offset 偏移量，查询数据使用
- watermark 
- LEO 分区副本的最新的消息id
## 性能
### 写入
- 顺序写入
- zero-copy 直接从内核态复制到用户态，或者是socket中
- 先写入内存，然后flush到磁盘
- 每个topic生成一个目录存放数据，磁盘存储主要是index 和log ，log是具体数据，index存储稀疏的offset，index文件名主要是按照消费id最小来命名
### 读取
- 请求broker获取对应的分区存储文件，然后找到对应的topic目录，然后通过offset寻找大于此的文件名，然后找到index文件，在index文件中寻找消息的offset类似的索引，然后在log文件中开始查询
- 重平衡，在consumer group变化的时候会进行重新分配，策略是轮询，范围、粘性分配
## 具体问题
### 怎么实现数据不丢失
- 通过ack+副本机制来实现数据的高可用和不丢失，ack=0就是数据直接发出，不关心是否存储ack=1只本地节点返回ack=-1双全部节点返回
- 
### 怎么实现数据的消费次数
- 至多一次 获取数据直接提交offset，再消费，但是消费过程中会出现问题
- 精准一次 通过offset和消息id做唯一关联，并且保证操作的原子性
- 最少一次 数据处理完成在commit，但是在commit的时候报错，所以会重复消费，所以，最少一次
### 反压
- kafka使用pull的方式进行消息消费，这样可以自己根据自己的消费能力进行消费。

# hadoop

## 常见参数配置
- map端的环形数组大小，默认100m可以修改，主要是减少磁盘io，提高效率，增加环形缓冲区的缓存比例，80%到90%
- 小文件合并，主要是减少io，提前做Combiner 减少io
- namenode工作线程池的大小设置
- dfs.namenode.http-address:50070
- dfs.datanode.address:50010
- yarn.resourcemanager.webapp.address:8088
## MR


- mapreduce是 的combine是减少传输到reduce端的数据量，主要是通过map本地的汇总来实现

- m实现kv产出 r实现k list<v>做简化
- MR过程中的排序（减少reduce的排序压力）
  * map  环形数组内存中排序一次（快速排序），溢出到本地文件，较多的小文件再做一次排序 （归并排序）
  * 分片读取数据etl，生成kv数据
  * reduce 阶段接受到map文件再内存中，如果内存够就内存排序一次，如果内存不够就本地再排序（ 归并排序）
- 聚合操作的时候需要shuffle
    * 输出格式 默认是文本输出格式 kv形式的
- map 优化
    * m，减少flush的次数，降低io次数，需要提高环形数组的大小（默认100M），输出压缩 spilt分片默认128

- shuffle
    * 在map端产出数据放在环形内存中，超过阈值，溢出为本地文件，在生成本地文件的时候根据reduce的个数进行分区排序。reduce端根据自己的分区号通过map的http服务拉取数据到本地，然后进行内存溢出文件合并
-reduce
   * reduce 也是减少flush的次数，降低io次数，还有就是combine，在map对聚合操作，减少传递的数据量




###  其他问题
- yarn管理资源，client向rm申请资源，rm分配contains运行 nm来管理contains的运行情况，am来监控应用的执行情况。主要调度方式有：fifo、公平、容量
- join 方式有map端 reduce端 半链接（map端过滤数据）
-percentile_approx（列，0.5中位值 可以使用median但是数据量有会超过限制
   * 多个计算最好在percentile_approx（列，,array(0.05,0.5,0.95)） 返回一个数组
- yarn的调度器有fifo、容量调度器，公平调度器
- hdfs的写先和namenode通信，获取datanode列表，然后进行datanode链接进行数据获取，客户端做合并操作， 最终获取文件
   * hadoop1和hadoop2的区别。加入yarn资源管理，做到资源和计算框架的分离，通过yarn申请资源
- 压缩gzip 
   * hdfs的写是客户端通过和namenode通信，然后获取datanode信息，客户端和datanode进行通信，将数据按照128一个大小分割成block然后发送给datanode，多个datanode串行通信，依次ack确认，然后反馈给客户端，继续下一次操作
### 优化
####  存储优化
- 生命周期减少
- 冷数据压缩
- 保留特定分区数据
#### 计算优化
- map实例较多，可以使用SET odps.sql.mapper.split.size=3000;调整mapper处理的文件大小 
- 数据倾斜 skewjoin解决倾斜问题，作用是对倾斜key做再随机处理，降低倾斜的概率
- map、reduce没有倾斜问题，处理较慢，主要是机器数比较少，可以使用SET odps.sql.joiner.instances =5000; 调整
- 多层group by 数据倾斜 可以做两层的groupby操作
-  使用cte简化重复代码的使用
-  使用原生udf提高性能
-  spilt_part 代替spilt主要是正则的消耗
-  json获取使用json_tuple
-  去重使用groupby ，因为distinct 会在一个reduce上
-  多层distinct会将数据量膨胀，所以可以将列在上一次做聚合
# spark
- reduce by key 和group by key的区别
- reduce 可以再map端进行聚合减少后续的数据量，提高性能
- groupby 是要在reduce端做处理，数据量较大
## 原理 
## 优化手段
 - spark.sparkContext.union(seq)进行操作，会减少内存占用
 - 
## 常见问题
 - driver运行在本地要考虑内存问题，否则会出现问题
 - spark-submit 可以执行yarn-cluster运行，在java或者scala的api下实现的，不行
 - driver 运行
 - df.createOrReplaceGlobalTempView 执行的全局临时表会导致以下的问题，只有在同一个sprak-session中才可以，否则会无法访问
 - spark 运行文件不存在，是因为spark需要将本地的文件上传到hdfs上，但是没有上传成功，会导致程序无法启动，报文件不存在，需要排查为什么不能提交文件，
 - Container exited with a non-zero exit code 1  可能是jar包配置参数有问题，不生效
 - 137退出码，可能是被内部kill等
 - Attribute(s) with the same name appear in the operation: rnum spark union字段的时候需要取别名，否则会出现问题
 
 

 
  
